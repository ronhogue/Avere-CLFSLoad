#
# clfsload/parse.py
#
#-------------------------------------------------------------------------
# Copyright (c) Microsoft.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#--------------------------------------------------------------------------

from __future__ import absolute_import
from __future__ import division

import collections
import io
import logging
import os
import struct
import time
import uuid

import lz4.frame

from clfsload.clfsutils import fnv1_hash, generate_bucket_name, hash64, time_secs_nsecs
from clfsload.types import Btype, CLFSCommonObjectId, CLFSCompressionType, \
                           CLFSEncryptionType, CLFSObjHandleType, CLFSObjectBlob, \
                           DEVICE_FTYPES, FILEHANDLE_NULL, FILEHANDLE_ORPHAN, FILEHANDLE_ROOT, \
                           Filehandle, Ftype, HandleBase, NamedObjectError, ObCacheId, \
                           SimpleError, TargetObj, TargetObjState, TargetObjectError, \
                           TerminalError
from clfsload.util import STRUCT_LE_U16, STRUCT_LE_U32, exc_info_err, exc_log

UUID_LEN = 16

class CLFSSegment():
    FIRST_SEGMENT_BYTES = 16384
    OTHER_SEGMENT_BYTES = 8388608
    DIR_OTHER_SEGMENT_BYTES = 262144
    MAX_SEGMENT_BYTES = max(FIRST_SEGMENT_BYTES, OTHER_SEGMENT_BYTES, DIR_OTHER_SEGMENT_BYTES)
    DIRECT_BLOCKS = 1024
    MAX_INDIR_DEPTH = 4
    INDIR_POINTERS_PER_BLOCK = 4096 # number of block pointers per indirect block.
    # number of data block pointers in trees 0 through 3
    INDIR_TREE_ITEMS = [4096, 4096*4096, 4096*4096*4096, 4096*4096*4096*4096]
    NULL_INDIR_LIST = [bytes(16), bytes(16), bytes(16), bytes(16)]

class CLFSBucketObjAttrs():
    '''
    Does not include bucketUUID, which must generated at obj creation time and
    added to extattr_dict.
    '''
    _EXTATTR_DICT = {'bucketCurrentNameVersion' : '2',
                     'bucketNameVersion' : '2',
                     'bucketObjVersion' : '2',
                    }

    def __init__(self):
        self._extattr_dict = dict(self._EXTATTR_DICT)
        self._extattr_dict['bucketUUID'] = str(uuid.uuid4())

    @property
    def extattr_dict(self):
        return dict(self._extattr_dict)

class ClfsObjParseBase():
    '''
    CLFS blob handle header-related constants in src/cloud/obtype.h
    base class for CLFS object handles (blob) in a single CLFS object
    '''
    OBTYPE_HEADER_BYTES = 20
    OBTYPE_MAGIC_20 = 0x03016005 # 05600103 packed little-endian
    OBTYPE_ROUNDUP_SIZE = 8
    OBTYPE_ROUNDUP_MASK = 0xFFFFFFF8
    VATTR_BYTE_COUNT = 104

def get_byte_count_header():
    return ClfsObjParseBase.OBTYPE_HEADER_BYTES

_HEADER_PACK1 = '<IIIII'

def unparse_header(ba, handleType, realCount, blobCount):
    '''
    return header as bytes array in network order
    realCount is the number of bytes required to store a particular blob type
    blobCount = HEADER_BYTES+realCount rounded up to 8-byte boundary
    paddingBytes = blobCount - realCount - OBTYPE_HEADER_BYTES
    '''
    ba.extend(struct.pack(_HEADER_PACK1, ClfsObjParseBase.OBTYPE_MAGIC_20, blobCount, realCount, handleType, 0))

def get_byte_count_attr():
    '''
    getByteCount* methods return the realCount given the contents of the blob
    number of bytes generated by unparse* methods must match the count generated by getByteCount* methods
    from definitions of CfsDiskVattr in src/cfs/cfsdisk.h
    cfsmode: 2 bytes
    ftype: 1 byte
    spare: 1 byte
    flags: 4 bytes
    uid: 4 bytes
    gid: 4 bytes
    length: 8 bytes
    dtime: 8 bytes
    mtime: 8 bytes
    ctime: 8 bytes
    atime: 8 bytes
    used:  8 bytes
    fsid:  8 bytes
    fileid: 8 bytes
    CfsSpecData _rdev: 8 bytes
    nlink: 4 bytes
    haVersion: 4 bytes
    union: 8 bytes

    4-byte prefix for length of the following attrs
    4-byte length + 104 bytes of attrs
    '''
    return ClfsObjParseBase.VATTR_BYTE_COUNT+4

_ATTR_PACK1 = '<HBBIIIQLLLLLLLLQQQIIIIQ'

def unparse_attr(run_options, ba, tObj):
    ba.extend(struct.pack(STRUCT_LE_U32, ClfsObjParseBase.VATTR_BYTE_COUNT))
    mt_sec, mt_nsec = time_secs_nsecs(tObj.mtime)
    ct_sec, ct_nsec = time_secs_nsecs(tObj.ctime)
    at_sec, at_nsec = time_secs_nsecs(tObj.atime)
    ba.extend(struct.pack(_ATTR_PACK1,
                          tObj.mode,
                          Btype.ftype_to_btype(tObj.ftype),
                          0, # spare
                          0, # flags
                          tObj.uid,
                          tObj.gid,
                          tObj.size,
                          0, 0, # dtime sec, nsec
                          mt_sec, mt_nsec,
                          ct_sec, ct_nsec,
                          at_sec, at_nsec,
                          tObj.size, # used
                          run_options.fsid,
                          tObj.source_inode_number,
                          os.major(tObj.dev),
                          os.minor(tObj.dev),
                          tObj.nlink_effective(),
                          0, # ignored by dirmgr during populate
                          run_options.containerid))

def get_byte_count_extattrs(attrdict):
    '''
    Extended attributes are a dictionary of str -> str
    (key, value) tuples
    '''
    size = 4 # number of attrs
    for key, value in attrdict.items():
        size += 4 + len(bytes(key, 'utf-8'))
        size += 4 + len(bytes(value, 'utf-8'))
    return size

def unparse_buf(ba, buffer):
    ba.extend(struct.pack(STRUCT_LE_U32, len(buffer)))
    ba.extend(buffer)

def unparse_extattrs(ba, attrdict):
    numattrs = len(attrdict)
    ba.extend(struct.pack(STRUCT_LE_U32, numattrs))
    for key, value in attrdict.items():
        unparse_buf(ba, bytes(key, 'utf-8'))
        unparse_buf(ba, bytes(value, 'utf-8'))

def get_byte_count_data(dataBa):
    '''
    4-byte length (in bytes) prefix followed by bytearray of data
    '''
    return len(dataBa) + 4

def unparse_data(ba, data):
    'Input: data as bytearray  out: bytearray ba'
    ba.extend(struct.pack(STRUCT_LE_U32, len(data)))
    ba.extend(data)

def get_byte_count_dirents_list(direntList):
    'compute bytes required for on-disk representation of dirents'
    count = 0
    for dirent in direntList:
        # 6 = 4 for length of dirent[0], 2 for length of dirent[1]
        count += 6
        if isinstance(dirent[0], str):
            dirent[0] = bytes(dirent[0], 'utf-8')
        count += len(dirent[0])
        count += len(dirent[1])
    return count

def unparse_dirents_list(ba, direntList):
    'unparse entries in dirent list into ba'
    for dirent in direntList:
        if isinstance(dirent[0], str):
            n = bytes(dirent[0], 'utf-8')
        else:
            n = dirent[0]
        ba.extend(struct.pack('<I%ds' % (len(n),), len(n), n))
        if isinstance(dirent[1], HandleBase):
            fhbytes = dirent[1].bytes
        else:
            assert isinstance(dirent[1], (bytes, bytearray))
            fhbytes = dirent[1]
        ba.extend(struct.pack('<H%ds' % (len(fhbytes,)), len(fhbytes), fhbytes))

def get_byte_count_back(backPtrFileHandleList):
    _count = 4  # size of back pointer list
    for backFh in backPtrFileHandleList:
        _count += 2 + len(backFh) # size of fh + 2
    return _count

def unparse_back(ba, backFhList):
    ba.extend(struct.pack(STRUCT_LE_U32, len(backFhList)))
    for backFh in  backFhList:
        ba.extend(struct.pack('<H%ds' % (len(backFh),), len(backFh), backFh.bytes))

def get_byte_count_name(nameStr):
    'Name blob -- name blob in cloud file/dir inode object'
    return 4 + len(nameStr.encode('utf-8'))

def unparse_name(ba, nameStr):
    nameba = str.encode(nameStr, "UTF-8")
    ba.extend(struct.pack(STRUCT_LE_U32, len(nameba)))
    ba.extend(nameba)

def get_byte_count_databack(ownerfh):
    '''
    databack blob -- points to owning file (if data block) or parent directory (if directory inode object)
    (CreateTime (4 bytes), BlockOffset (or indirect block id)(8 bytes), ownerfh)
    '''
    return 4 + 8 + (2 + len(ownerfh))

_UNPARSE_DATABACK_PACK1 = "<IQ"
def unparse_databack(ba, ctime, blockOffset, ownerfh):
    if isinstance(ownerfh, HandleBase):
        ownerfh = ownerfh.bytes
    ba.extend(struct.pack(_UNPARSE_DATABACK_PACK1, ctime, blockOffset))
    ba.extend(struct.pack("<H%ds" % (len(ownerfh),), len(ownerfh), ownerfh))

def get_byte_count_bmap(directBlockList, indirectBlockList):
    fhlen = UUID_LEN # block ids are UUIDs (except for inode obj)
    assert len(directBlockList) <= CLFSSegment.DIRECT_BLOCKS
    count = 16 # first segment and other segment bytes followed by direct block count, length of indirectBlockTreeRootList comes later
    for crs in directBlockList:
        count += 2 + len(crs) # 2byte length + fh bytearray len
    assert len(indirectBlockList) == 4
    count += len(indirectBlockList) * (fhlen + 2) # indirect-block-tree root block ids
    return count

_UNPARSE_BMAP_PACK1 = '<II'
def unparse_bmap(ba, firstSegBytes, otherSegBytes, directBlockList, indirectBlockTreeRootList):
    assert len(indirectBlockTreeRootList) == 4
    ba.extend(struct.pack(_UNPARSE_BMAP_PACK1, firstSegBytes, otherSegBytes))
    unparse_block_list(ba, directBlockList)
    unparse_block_list(ba, indirectBlockTreeRootList)

def unparse_block_list(ba, blks):
    '''
    Write a list of block IDs
    '''
    ba.extend(struct.pack(STRUCT_LE_U32, len(blks)))
    for blk in blks:
        try:
            blk = blk.bytes
        except AttributeError:
            pass
        ba.extend(struct.pack("<H%ds" % len(blk), len(blk), blk))

_PARSE_BMAP_PACK1 = '<III'

def get_byte_count_indir(indirectBlockList):
    '''
    We assume all indirect block pointers (including null blocks) are passed as input to parse/getByteCount
    count of block pointers (4 bytes) and (4 byte length + filehandle bytearray) for each block id
    '''
    return 4 + (len(indirectBlockList) * (2 + len(uuid.uuid4().bytes)))

def unparse_indir(ba, indirectBlockList):
    blockIdLen = len(uuid.uuid4().bytes)
    ba.extend(struct.pack(STRUCT_LE_U32, len(indirectBlockList)))
    for blk in indirectBlockList:
        try:
            blk = blk.bytes
        except AttributeError:
            pass
        assert len(blk) == blockIdLen
        ba.extend(struct.pack("<H%ds" % (blockIdLen,), blockIdLen, blk))

_COMPRESSION_HEADERS_PACK1 = '<HH'
def get_compress_mode(ba):
    '''
    Return a tuple of encryption_mode, compression_mode, bytes_remaining
    '''
    return struct.unpack(_COMPRESSION_HEADERS_PACK1+str(len(ba)-4)+'s', ba)

_GET_UNPARSE_HANDLE_LIST__REGblob = (CLFSObjHandleType.OBTYPE_VATTR,
                                     CLFSObjHandleType.OBTYPE_BMAP,
                                     CLFSObjHandleType.OBTYPE_BACK,
                                     CLFSObjHandleType.OBTYPE_NAME,
                                     CLFSObjHandleType.OBTYPE_DATA,
                                    )
_GET_UNPARSE_HANDLE_LIST__DIRblob = (CLFSObjHandleType.OBTYPE_VATTR,
                                     CLFSObjHandleType.OBTYPE_BMAP,
                                     CLFSObjHandleType.OBTYPE_NAME,
                                     CLFSObjHandleType.OBTYPE_DIRENTS,
                                    )
_GET_UNPARSE_HANDLE_LIST__SEGBLOB_DIR = (CLFSObjHandleType.OBTYPE_DIRENTS,
                                         CLFSObjHandleType.OBTYPE_DATA,
                                         CLFSObjHandleType.OBTYPE_DATABACK,
                                        )
_GET_UNPARSE_HANDLE_LIST__SEGBLOB_NONDIR = (CLFSObjHandleType.OBTYPE_DATA,
                                            CLFSObjHandleType.OBTYPE_DATABACK,
                                           )
_GET_UNPARSE_HANDLE_LIST__INDblob = (CLFSObjHandleType.OBTYPE_INDIR,
                                     CLFSObjHandleType.OBTYPE_DATABACK,
                                    )
_GET_UNPARSE_HANDLE_LIST__MKNODlike = (Btype.BTYPE_BLK, Btype.BTYPE_CHR, Btype.BTYPE_SOCK, Btype.BTYPE_FIFO)
_GET_UNPARSE_HANDLE_LIST__MKNODblob = (CLFSObjHandleType.OBTYPE_VATTR,
                                       CLFSObjHandleType.OBTYPE_BMAP,
                                       CLFSObjHandleType.OBTYPE_BACK,
                                       CLFSObjHandleType.OBTYPE_NAME,
                                      )
_GET_UNPARSE_HANDLE_LIST__SPECblob = (CLFSObjHandleType.OBTYPE_EXTATTRS,
                                     )

_GET_UNPARSE_HANDLE_LIST__DICT = {Btype.BTYPE_REG : _GET_UNPARSE_HANDLE_LIST__REGblob,
                                  Btype.BTYPE_LNK : _GET_UNPARSE_HANDLE_LIST__REGblob,
                                  Btype.BTYPE_HLINK : _GET_UNPARSE_HANDLE_LIST__REGblob,
                                  Btype.BTYPE_DIR : _GET_UNPARSE_HANDLE_LIST__DIRblob,
                                  Btype.BTYPE_INDIRECT : _GET_UNPARSE_HANDLE_LIST__INDblob,
                                  Btype.BTYPE_BLK : _GET_UNPARSE_HANDLE_LIST__MKNODblob,
                                  Btype.BTYPE_CHR : _GET_UNPARSE_HANDLE_LIST__MKNODblob,
                                  Btype.BTYPE_SOCK : _GET_UNPARSE_HANDLE_LIST__MKNODblob,
                                  Btype.BTYPE_FIFO : _GET_UNPARSE_HANDLE_LIST__MKNODblob,
                                  Btype.BTYPE_SPECIAL : _GET_UNPARSE_HANDLE_LIST__SPECblob, # only deal with CLFSCommonObjectId.BUCKETID,
                                 }

def get_unparse_handle_list(ftype, objBtype):
    if objBtype == Btype.BTYPE_SEGMENT:
        if ftype == Ftype.DIR:
            return _GET_UNPARSE_HANDLE_LIST__SEGBLOB_DIR
        return _GET_UNPARSE_HANDLE_LIST__SEGBLOB_NONDIR
    try:
        return _GET_UNPARSE_HANDLE_LIST__DICT[objBtype]
    except KeyError:
        raise SimpleError("cannot unparse obj with unknown ftype %s" % ftype)

def _uncompressed_obj(inbuf):
    header = struct.pack(_COMPRESSION_HEADERS_PACK1, CLFSEncryptionType.DISABLED, CLFSCompressionType.DISABLED)
    return bytes().join([header, inbuf])

def _compress_obj(inbuf, wrock):
    '''
    wrapper function to compress blocks in 256K chunks that are interdependent
    MAXFRAMESIZE = 262144 # 256K to match src/lz4/lz4wrap.cc in armada source tree
    returns compressed buffer.
    '''
    inlen = len(inbuf)
    # We could look at inlen here and short-circuit out if
    # it is below some threshold where we do not believe compression
    # helps. In reality, because we always have various internal
    # headers, such a check either generates false positives or
    # never triggers.

    header = struct.pack(_COMPRESSION_HEADERS_PACK1, CLFSEncryptionType.DISABLED, CLFSCompressionType.LZ4)
    header_len = len(header)
    bufs = [header]

    max_compressed_len = inlen + header_len

    compress_ctx = lz4.frame.create_compression_context()
    buf = lz4.frame.compress_begin(compress_ctx, block_size=lz4.frame.BLOCKSIZE_MAX256KB, source_size=False)
    bufs.append(buf)
    bufs_len = header_len + len(buf)
    cursor = 0
    while cursor < inlen:
        block_length = min(inlen-cursor, CLFSCompressionType.MAXFRAMESIZE)
        compress_block = lz4.frame.compress_chunk(compress_ctx, inbuf[cursor:cursor+block_length])
        bufs_len += len(compress_block)
        if bufs_len >= max_compressed_len:
            # if compression does not help, skip it
            return _uncompressed_obj(inbuf)
        if len(compress_block) >= 4:
            (length,) = struct.unpack(STRUCT_LE_U32, compress_block[:4])
            # highest bit of 4-byte integer is set when block is uncompressed.
            # armada/src/lz4/lz4wrap.cc cannot handle this and will fail with bad block size.
            # If any block is uncompressed, then do not compress object
            if length > 0x7FFFFFFF:
                return _uncompressed_obj(inbuf)
        bufs.append(compress_block)
        cursor += block_length
    buf = lz4.frame.compress_flush(compress_ctx, end_frame=True)
    bufs.append(buf)
    bufs_len += len(buf)
    if bufs_len > max_compressed_len:
        # if compressed obj is larger than uncompressed obj, skip compression
        return _uncompressed_obj(inbuf)
    wrock.stats.stat_inc('compress_blob')
    return bytes().join(bufs)

def add_compression_header(buffer, wrock, do_compress):
    if do_compress:
        return _compress_obj(buffer, wrock)
    return _uncompressed_obj(buffer)

def strip_compression_header_and_decompress(ba, tobj, blob_name):
    '''
    strip 4-byte compression/encryption-type header
    uncompress buffer if header indicates it is compressed
    '''
    emode, cmode, ba = get_compress_mode(ba)
    if emode != CLFSEncryptionType.DISABLED:
        err = "unexpected emode %s %s" % (emode.__class__.__name__, emode)
        raise TargetObjectError(tobj, err, blob_name=blob_name)
    if cmode == CLFSCompressionType.DISABLED:
        pass
    elif cmode == CLFSCompressionType.LZ4: #LZ4HC is not supported by armada_main
        ba = lz4.frame.decompress(ba)
    else:
        err = "unexpected cmode %s %s" % (cmode.__class__.__name__, cmode)
        raise TargetObjectError(tobj, err, blob_name=blob_name)
    return ba

def unparse_obj_handles(run_options,
                        tobj,
                        ba,
                        afh,
                        objBtypeList=None,
                        targetObj=None,
                        ownerFh=None,
                        dataOffset=None,
                        dataBa=None,
                        direntDataBa=None,
                        direntList=None,
                        directBlockList=None,
                        indirectBlockList=None,
                        backPointerList=None,
                        extattrDict=None,
                        targetName=None):
    '''
    return a byte array of data that can be put in container and is a valid
    clfs inode object. Not that bytearray returned does not have the
    4-byte compression+encryption header
    objBtype: type of object to be unparsed
    tObj: targetObj
    ownerfh: owning fh if datablock or parent dir if directory inode
    dataOffset: offset of data in file
    direntDataba: bytearray of dirent data for dir segments. len(bytes) is number of bytes to be written
    databa: bytearray of data. len(bytes) is number of bytes to be written
    direntList: list of (name,fh) tuples
    directBlockList: list of direct blocks in indices 0 to 1023
    indirectBlockList: list of indirect block pointer to the root of the indirect block trees
    backPointerList: list of parent points for a non-directory object
    extattrDict: dictionary of extended attributes and their values
    '''

    for objHandleType in objBtypeList:
        if objHandleType == CLFSObjHandleType.OBTYPE_DATA:
            realCount = get_byte_count_data(dataBa)
        elif objHandleType == CLFSObjHandleType.OBTYPE_DIRENTS:
            if direntList:
                realCount = get_byte_count_dirents_list(direntList)
            elif direntDataBa is not None:
                realCount = len(direntDataBa)
            else:
                raise TargetObjectError(tobj, "no direntList or direntDataBa for OBTYPE_DIRENTS (internal error)")
        elif objHandleType == CLFSObjHandleType.OBTYPE_VATTR:
            realCount = get_byte_count_attr()
        elif objHandleType == CLFSObjHandleType.OBTYPE_BMAP:
            if len(directBlockList) > CLFSSegment.DIRECT_BLOCKS:
                raise TargetObjectError(tobj, "DirectBlockList Too Large: %d limit: %d" % (len(directBlockList), CLFSSegment.DIRECT_BLOCKS))
            realCount = get_byte_count_bmap(directBlockList, indirectBlockList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_INDIR:
            realCount = get_byte_count_indir(indirectBlockList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_BACK:
            realCount = get_byte_count_back(backPointerList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_DATABACK:
            realCount = get_byte_count_databack(ownerFh)
        elif objHandleType == CLFSObjHandleType.OBTYPE_EXTATTRS:
            realCount = get_byte_count_extattrs(extattrDict)
        elif objHandleType == CLFSObjHandleType.OBTYPE_NAME:
            realCount = get_byte_count_name(targetName)
        else:
            raise TargetObjectError(tobj, "blob byteCount not implemented for objHandleType %s %s" % (objHandleType.__class__.__name__, objHandleType))

        blobCountRaw = realCount + get_byte_count_header()
        blobCount = (blobCountRaw + ClfsObjParseBase.OBTYPE_ROUNDUP_SIZE - 1) & ClfsObjParseBase.OBTYPE_ROUNDUP_MASK
        padding = blobCount - blobCountRaw
        unparse_header(ba, objHandleType.value, realCount, blobCount)

        if objHandleType == CLFSObjHandleType.OBTYPE_DATA:
            unparse_data(ba, dataBa)
        elif objHandleType == CLFSObjHandleType.OBTYPE_DIRENTS:
            if direntList:
                unparse_dirents_list(ba, direntList)
            else:
                # we verfied above that we have either direntList or direntDataBa
                ba.extend(direntDataBa)
        elif objHandleType == CLFSObjHandleType.OBTYPE_VATTR:
            unparse_attr(run_options, ba, targetObj)
        elif objHandleType == CLFSObjHandleType.OBTYPE_BMAP:
            if targetObj.ftype == Ftype.DIR:
                otherSegBytes = CLFSSegment.DIR_OTHER_SEGMENT_BYTES
            else:
                otherSegBytes = CLFSSegment.OTHER_SEGMENT_BYTES
            assert afh == targetObj.filehandle
            unparse_bmap(ba, CLFSSegment.FIRST_SEGMENT_BYTES, otherSegBytes, directBlockList, indirectBlockList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_INDIR:
            unparse_indir(ba, indirectBlockList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_BACK:
            unparse_back(ba, backPointerList)
        elif objHandleType == CLFSObjHandleType.OBTYPE_DATABACK:
            unparse_databack(ba, int(time.time()), dataOffset, ownerFh)
        elif objHandleType == CLFSObjHandleType.OBTYPE_EXTATTRS:
            unparse_extattrs(ba, extattrDict)
        elif objHandleType == CLFSObjHandleType.OBTYPE_NAME:
            unparse_name(ba, targetName)
        else:
            raise TargetObjectError(tobj, "blob unparse not implemented for objHandleType %s %s" % (objHandleType.__class__.__name__, objHandleType))

        if padding > 0:
            ba.extend(bytes(padding))

    return ba

class ParseState():
    '''
    Track state while parsing a single object
    '''
    def __init__(self, tobj, ba):
        self._tobj = tobj
        self._bio = io.BytesIO(ba)
        self.parseDict = dict()
        self._magic = None
        self._blobCount = None
        self._realCount = None
        self._objHandleType = None
        self._last_read_length = None

    def resume_from_oblob(self, oblob):
        '''
        Update parser state for the given oblob.
        For performance, assumes that this object
        was initialized with oblob.data for ba.
        '''
        self._magic = ClfsObjParseBase.OBTYPE_MAGIC_20
        self._blobCount = oblob.blobCount
        self._realCount = len(oblob.data)
        self._objHandleType = oblob.obtype

    def _read(self, length):
        '''
        Read and return up to length bytes. Returns zero-length
        bytes when there is nothing left.
        '''
        ret = self._bio.read(length)
        self._last_read_length = len(ret)
        if self._last_read_length != length:
            raise TargetObjectError(self._tobj, "short read %d != %d" % (len(ret), length))
        return ret

    def _read_with_length16(self):
        '''
        Read a length and a buffer of that length and return it
        '''
        length = struct.unpack(STRUCT_LE_U16, self._read(2))[0]
        return self._read(length)

    def _read_with_length32(self):
        '''
        Read a length and a buffer of that length and return it
        '''
        length = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        return self._read(length)

    def _read_name(self, *args):
        '''
        Read and decode a name
        '''
        name = self._read_with_length32()
        return name.decode(*args)

    def _parse_header(self):
        '''
        Read and parse the next header. Returns whether or not
        a header is found.
        '''
        header_bytes = self._bio.read(ClfsObjParseBase.OBTYPE_HEADER_BYTES)
        self._last_read_length = len(header_bytes)
        if not header_bytes:
            return False
        if self._last_read_length != ClfsObjParseBase.OBTYPE_HEADER_BYTES:
            raise TargetObjectError(self._tobj, "short read %d != %d" % (len(header_bytes), ClfsObjParseBase.OBTYPE_HEADER_BYTES))
        self._magic, self._blobCount, self._realCount, handleType, _ = struct.unpack(_HEADER_PACK1, header_bytes)
        if self._magic != ClfsObjParseBase.OBTYPE_MAGIC_20:
            raise TargetObjectError(self._tobj, "header magic %s != %s" % (oct(self._magic), oct(ClfsObjParseBase.OBTYPE_MAGIC_20)))
        try:
            self._objHandleType = CLFSObjHandleType(handleType)
        except ValueError:
            raise TargetObjectError(self._tobj, "invalid handle type '%s'" % handleType)
        return True

    def _parse_data(self):
        datasize = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        self.parseDict['Data'] = self._read(datasize)

    def parse_dirents(self):
        '''
        parse (name,filehandle) pairs and add them to DirentList.
        Use realCount to determine when to terminate parsing.
        '''
        bytecount = self._realCount
        direntList = list()
        count = 0
        while count < bytecount:
            count += 6 # 4 for name length then 2 for fh length
            name = self._read_name()
            count += self._last_read_length
            fh = self._read_with_length16()
            count += self._last_read_length
            direntList.append([name, fh])
        self.parseDict['DirentList'] = direntList
        return direntList

    def _parse_attr(self):
        v = struct.unpack(_ATTR_PACK1, self._read_with_length32())
        btype = v[1]
        ftype = Btype.btype_to_ftype(btype)
        device = os.makedev(v[18], v[19]) if ftype in DEVICE_FTYPES else 0
        tObj = TargetObj(filehandle=self._tobj.filehandle,
                         mode=v[0],
                         ftype=ftype,
                         # spare = v[2]
                         # flags = v[3]
                         uid=v[4],
                         gid=v[5],
                         size=v[6],
                         # dtime = v[7]
                         mtime=v[9],
                         ctime=v[11],
                         atime=v[13],
                         # used = v[15]]
                         #fsid=v[16],
                         source_inode_number=v[17],
                         dev=device,
                         nlink=v[20],
                         first_backpointer=Filehandle(),
                         state=TargetObjState.DONE,
                         source_path='')
        self.parseDict['Obj'] = tObj

    def _parse_bmap(self):
        parseDict = self.parseDict
        fsegBytes, otherSegBytes, directListSize = struct.unpack(_PARSE_BMAP_PACK1, self._read(12))
        parseDict['FirstSegmentBytes'] = fsegBytes
        parseDict['OtherSegmentBytes'] = otherSegBytes
        parseDict['DirectBlocks'] = [self._read_with_length16() for _ in range(directListSize)]
        indirListSize = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        if indirListSize != 4: # sanity check -- making indir size > 4 is not supported.
            raise TargetObjectError(self._tobj, "indirListSize has unexpected value %s" % indirListSize)
        parseDict['IndirectBlockTrees'] = [self._read_with_length16() for _ in range(indirListSize)]

    def _parse_indir(self):
        numblocks = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        self.parseDict['IndirectBlocks'] = [self._read_with_length16() for _ in range(numblocks)]

    def _parse_back(self):
        numBack = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        self.parseDict['BackPointers'] = [Filehandle(fh) for fh in [self._read_with_length16() for _ in range(numBack)]]

    def _parse_databack(self):
        ctime, blockOffset, fhlen = struct.unpack('<IQH', self._read(14))
        ownerfh = self._read(fhlen)
        self.parseDict['DataBack'] = [ctime, blockOffset, ownerfh]

    def _parse_extattrs(self):
        '''
        4 bytes for number of <key,value> pairs of extended attrs
        where key and value are both strings.
        '''
        numattrs = struct.unpack(STRUCT_LE_U32, self._read(4))[0]
        # Logically, it seems like this could be:
        #    attrdict = {self._read_name('UTF-8') : self._read_name('UTF-8') for _ in range(numattrs)}
        # That will not work, however, because Python fills the
        # value before the key.
        attrdict = dict()
        for _ in range(numattrs):
            key = self._read_name('UTF-8')
            value = self._read_name('UTF-8')
            attrdict[key] = value
        self.parseDict['ExtAttrs'] = attrdict

    def _parse_name(self):
        self.parseDict['Name'] = self._read_name('UTF-8')

    def parse(self):
        '''
        Iterate through the CLFS blobs loading the logical
        contents into self.parseDict
        '''
        while self._parse_header():
            try:
                parseFunc = self._PARSE_DICT[self._objHandleType]
            except KeyError:
                raise TargetObjectError(self._tobj, "unrecognized objHandleType %s" % self._objHandleType)
            parseFunc(self)
            padding = self._blobCount - self._realCount - ClfsObjParseBase.OBTYPE_HEADER_BYTES
            self._bio.seek(padding, io.SEEK_CUR)

    _PARSE_DICT = {CLFSObjHandleType.OBTYPE_DATA : _parse_data,
                   CLFSObjHandleType.OBTYPE_DIRENTS : parse_dirents,
                   CLFSObjHandleType.OBTYPE_VATTR : _parse_attr,
                   CLFSObjHandleType.OBTYPE_BMAP : _parse_bmap,
                   CLFSObjHandleType.OBTYPE_INDIR : _parse_indir,
                   CLFSObjHandleType.OBTYPE_BACK : _parse_back,
                   CLFSObjHandleType.OBTYPE_DATABACK : _parse_databack,
                   CLFSObjHandleType.OBTYPE_EXTATTRS : _parse_extattrs,
                   CLFSObjHandleType.OBTYPE_NAME : _parse_name,
                  }

    def parse_shallow(self):
        '''
        Parse out each clfs object blob.
        '''
        ret = list()
        while self._parse_header():
            data = self._read(self._realCount)
            ret.append(CLFSObjectBlob(self._objHandleType, data, self._blobCount, self._realCount))
            blobCountRaw = self._realCount + get_byte_count_header()
            blobCount = (blobCountRaw + ClfsObjParseBase.OBTYPE_ROUNDUP_SIZE - 1) & ClfsObjParseBase.OBTYPE_ROUNDUP_MASK
            padding = blobCount - blobCountRaw
            self._bio.seek(padding, io.SEEK_CUR)
        return ret

def parse_obj(tobj, ba):
    '''
    ba is the uncompressed object after 4-byte compression+encryption header has been stripped
    '''
    parseState = ParseState(tobj, ba)
    parseState.parse()
    return parseState.parseDict

def unparse_segment(run_options, tobj, ftype, ownerfh, offset, data):
    'pass owning object offset and data at offset'
    segmentId = ObCacheId()
    return segmentId, unparse_segment_using_obid(run_options, segmentId, tobj, ftype, ownerfh, offset, data)

def unparse_segment_using_obid(run_options, segmentId, tobj, ftype, ownerfh, offset, data):
    'pass owning object offset and data at offset'
    ba = bytearray()
    btypeList = get_unparse_handle_list(ftype, Btype.BTYPE_SEGMENT)
    # armada_main bug requires a null data segment in dirent segment, unfortunately
    if ftype == Ftype.DIR:
        dataBa = bytearray()
        direntDataBa = data
    else:
        dataBa = data
        direntDataBa = None
    ba = unparse_obj_handles(run_options,
                             tobj,
                             ba,
                             segmentId,
                             objBtypeList=btypeList,
                             ownerFh=ownerfh,
                             dataOffset=offset,
                             direntDataBa=direntDataBa,
                             dataBa=dataBa)
    return ba

def unparse_indirect(run_options, tObj, blockList, treeIndex, depth):
    ba = bytearray()
    btypeList = get_unparse_handle_list(tObj.ftype, Btype.BTYPE_INDIRECT)
    indirBlockId = ObCacheId()
    offset = 0xFFFFFFFF & ((treeIndex << 16) + depth)
    ba = unparse_obj_handles(run_options,
                             tObj,
                             ba,
                             indirBlockId,
                             objBtypeList=btypeList,
                             ownerFh=tObj.filehandle,
                             dataOffset=offset,
                             indirectBlockList=blockList)
    return indirBlockId, ba

def unparse_inode(run_options, tObj, data, dataBlocks, indirectBlocks, backPointerList, name):
    ba = bytearray()
    btypeList = get_unparse_handle_list(tObj.ftype, Btype.ftype_to_btype(tObj.ftype))

    # Root Dir object must have container epoch > 0 and have the wipeEpochUUID and massUUID set
    # This is required for containers that have pre-existing data when being added to a
    # cluster. Special-case this in the unparse function.
    extattrs = None
    if tObj.filehandle == FILEHANDLE_ROOT:
        btypeList = list(btypeList)
        btypeList.append(CLFSObjHandleType.OBTYPE_EXTATTRS)
        extattrs = {'containeridEpoch' : '2',
                    'massUUID' : str(uuid.uuid4()),
                    'wipeEpoch' : str(uuid.uuid4()),}

    if tObj.ftype == Ftype.DIR:
        dataBa = bytearray()
        direntDataBa = data
    else:
        dataBa = data
        direntDataBa = None
    ba = unparse_obj_handles(run_options,
                             tObj,
                             ba,
                             tObj.filehandle,
                             objBtypeList=btypeList,
                             targetObj=tObj,
                             ownerFh=tObj.first_backpointer,
                             dataOffset=0,
                             direntDataBa=direntDataBa,
                             dataBa=dataBa,
                             directBlockList=dataBlocks,
                             indirectBlockList=indirectBlocks,
                             backPointerList=backPointerList,
                             extattrDict=extattrs,
                             targetName=name)
    return ba

def generate_bucket_target_obj():
    bucketfh = Filehandle(CLFSCommonObjectId.BUCKETID)
    tobj = TargetObj(filehandle=bucketfh, #all args other than filehandle ignored
                     ftype=Ftype.DIR, #ignored
                     mode=0, #ignored
                     nlink=0,
                     mtime=0,
                     ctime=0,
                     atime=0,
                     size=0,
                     uid=0,
                     gid=0,
                     dev=0,
                     first_backpointer=FILEHANDLE_NULL,
                     state=TargetObjState.DONE,
                     source_inode_number=0,
                     source_path='bucket_obj')
    bname = generate_bucket_name(bucketfh, bucketfh, Btype.BTYPE_SPECIAL)
    return bname, tobj

class IndirectBlockItem():
    '''
    Class to maintain a single indirect block tuple: (block, blockObId, blockPointerList)
    blockPointerList is the list of ObCacheIds contained in the indirect block
    identified by blockObId. A tree of depth d has data blocks at depth d+1.
    blockDepth is a value in [0,d) in a tree of depth d, where 0 <= d < 4
    '''

    def __init__(self, blocks_committed, depth):
        self._block_depth = depth
        self._blocks_committed = blocks_committed
        self._block_list = list()
        # filled in only after unparse_segment has been called on this item
        self._block_id = None
        self._data = None

    def set_block_id_and_data(self, ibid, data):
        self._block_id = ibid
        self._data = data

    def get_blocks_committed(self):
        return self._blocks_committed + len(self._block_list)

    def get_block_id_and_data(self):
        return self._block_id, self._data

    def add_block_to_list(self, blockid):
        self._block_list.append(blockid)
        # return True if indirect block is full
        return len(self._block_list) == CLFSSegment.INDIR_POINTERS_PER_BLOCK

    def get_info(self):
        return self._block_depth, self._block_list

class BlockListManager():
    '''
    Class to build the block map for a file/dir object.
    This class maintains the list of UUIDs to be written into an object's blockmap
    append() method adds a new block to a map, and in the cases where blocks are
    in an indirect  tree, indirect blocks are created and flushed to the backend
    so that minimal state information is maintained. The goal is to maintain only
    the part of the indirect tree that will be updated when the next block is added.

    _current_depth_blocks maintains a set of (indirect_block_UUID, list of block pointer) pairs
    corresponding the unflushed part of the tree, so the number of block pointers maintained
    in memory for depth d is atmost  (d+1)*_POINTERS_PER_BLOCK, where 0 <= d <= 3. As each indirect
    block is flushed, the uuid of the block is added to the parent indirect block (if any). if the
    indirect block is the root of the tree, then move to the next indirect block, which is maintained
    in _current_depth.

    This caches and uses the passed-in wrock. Thus, only the thread that
    constructs this object may use it.
    '''
    def __init__(self, wrock, tobj):
        self._wrock = wrock
        self._run_options = wrock.run_options
        self._tobj = tobj
        self._treeroots = list(CLFSSegment.NULL_INDIR_LIST) # list of UUIDs corresponding to root of tree.
        self._flush_deque = collections.deque()
        self._current_depth = 0 # Always start at 0-th indirect block tree
        self._direct_blocks = list()
        self._current_depth_blockitems = [IndirectBlockItem(0, 0)]
        self._treeblockcount = 0
        self._tree_block_counts = list() # total number of block counts in previous trees

    def get_direct_blocks(self):
        return self._direct_blocks

    def get_indirect_root_list(self):
        return self._treeroots

    def get_committed_counts(self):
        '''
        counts number of data blocks committed at the next depth
        so count c at dept d is the number of blocks flushed at depth d+1
        '''
        return self._tree_block_counts

    def _maybe_flush_block(self, newBlockId=None, allBlocksDone=False):
        '''
        When all blocks of a file have been flush and remaining data blocks need
        to be flushed, this method flushes any unflushed indirect blocks all the way
        to root of indirect block tree.

        Returns True if we need to move to next indirect tree; False otherwise
        '''
        for d in range(self._current_depth, -1, -1):
            flushitem = self._current_depth_blockitems[d]
            flushneeded = False
            if isinstance(newBlockId, ObCacheId):
                flushneeded = flushitem.add_block_to_list(newBlockId)
            if not flushneeded and not allBlocksDone:
                return False
            bdepth, blist = flushitem.get_info()

            # flush indirect block
            if d != bdepth:
                self._wrock.logger.error("afh %s depth mismatch: depth=%d flushitem_depth=%d current_depth=%d blocklist_size=%d",
                                         self._tobj.filehandle, d, bdepth, self._current_depth, len(blist))
                raise TargetObjectError(self._tobj, "depth mismatch in flush item")
            ibid, ibdata = unparse_indirect(self._run_options, self._tobj, blist, self._current_depth, bdepth)
            flushitem.set_block_id_and_data(ibid, ibdata)
            self._flush_deque.append(flushitem)

            if flushneeded and not allBlocksDone:
                # replace blockItem since the previous one is done and added to _flush_deque
                # number of block ids committed to storage is returned by flushitem.get_blocks_committed()
                # and is set to the _blocks_committed value for the newly created block.
                self._current_depth_blockitems[d] = IndirectBlockItem(flushitem.get_blocks_committed(), d)
            newBlockId = ibid
        if d == 0:
            self._treeroots[self._current_depth] = newBlockId
            # stash block counts in  current tree (for testing/debugging)
            counts = list()
            for dpt in range(0, self._current_depth+1):
                counts.append(self._current_depth_blockitems[dpt].get_blocks_committed())
            self._tree_block_counts.append(counts)
            return True
        return False

    def flush_indir_tree_final(self):
        '''
        There are no more blocks to add. Flush current tree
        return list of blocks to flush
        '''
        self._flush_deque = collections.deque()
        if self._treeblockcount > 0:
            haveBlocksToFlush = self._maybe_flush_block(allBlocksDone=True)
            assert haveBlocksToFlush
        return self._flush_deque

    def add_block(self, blockIndex, blockId, allDone=False):
        '''Given an ObCacheId blockId at a blockIndex in the map, add it to
        list of blocks. If number of block pointers reaches limit
        _POINTERS_PER_BLOCK Flush indirect block and update
        _current_depth, _current_depth_blocks, and
        _current_block_list.
        '''
        if blockIndex < CLFSSegment.DIRECT_BLOCKS:
            self._direct_blocks.append(blockId)
            return list()

        if len(self._direct_blocks) > CLFSSegment.DIRECT_BLOCKS:
            self._wrock.logger.error("too many directblocks for %s seen=%d max=%d",
                                     self._tobj.filehandle, len(self._direct_blocks), CLFSSegment.DIRECT_BLOCKS)
            raise TargetObjectError(self._tobj, "Too Many Direct Blocks")

        # starting on a new indirect tree. initialize IndirectBlockItems for all
        # non-leaf indir tree nodes.
        if not self._current_depth_blockitems:
            for d in range(0, self._current_depth+1):
                self._current_depth_blockitems.append(IndirectBlockItem(0, d))

        assert len(self._current_depth_blockitems) == self._current_depth+1

        self._flush_deque = collections.deque()
        self._treeblockcount += 1
        # if adding a new block results in one or more full indirect blocks,
        # self._flush_deque keeps track of the contents of these indirect blocks.
        if self._maybe_flush_block(newBlockId=blockId, allBlocksDone=allDone):
            # we move onto next tree
            assert self._treeblockcount == CLFSSegment.INDIR_TREE_ITEMS[self._current_depth]
            self._treeblockcount = 0
            self._current_depth += 1
            self._current_depth_blockitems = list()

        if self._current_depth >= CLFSSegment.MAX_INDIR_DEPTH:
            raise TargetObjectError(self._tobj, "Indirect tree too deep. File too large")

        return self._flush_deque

def obj_reconcile(wrock, tobj, content, desc, blob_name):
    '''
    Do the work to reconcile a target. tobj is the current
    database content with the backpointer map fully populated.
    content is the data payload of the object.
    desc is a description of the backing - eg 'blob' for Azure
    Return the new data payload for the object.
    '''
    try:
        ba = strip_compression_header_and_decompress(content, tobj, blob_name)
        parse_state = ParseState(tobj, ba)
        oblobs = parse_state.parse_shallow()
    except (NamedObjectError, TerminalError):
        raise
    except Exception:
        exc_log(wrock.logger, logging.ERROR, "inode %s parse failure", desc)
        raise TargetObjectError(tobj, "inode %s parse failure: %s" % (desc, exc_info_err()))
    vattr_bytes = bytearray()
    unparse_attr(wrock.run_options, vattr_bytes, tobj)
    backpointers = tobj.backpointer_list_generate(include_null_firstbackpointer=True)
    backpointer_bytes = bytearray()
    unparse_back(backpointer_bytes, backpointers)
    ba = bytearray()
    owner_id = None
    for oblob in oblobs:
        obtype = oblob.obtype
        if obtype == CLFSObjHandleType.OBTYPE_VATTR:
            data = vattr_bytes
        elif obtype == CLFSObjHandleType.OBTYPE_BACK:
            data = backpointer_bytes
        elif obtype == CLFSObjHandleType.OBTYPE_DIRENTS:
            data, owner_id = _obj_reconcile__repack_dirents(wrock, tobj, oblob, blob_name)
        else:
            data = oblob.data
        realCount = len(data)
        blobCountRaw = realCount + get_byte_count_header()
        blobCount = (blobCountRaw + ClfsObjParseBase.OBTYPE_ROUNDUP_SIZE - 1) & ClfsObjParseBase.OBTYPE_ROUNDUP_MASK
        padding = blobCount - blobCountRaw
        unparse_header(ba, obtype.value, realCount, blobCount)
        ba.extend(data)
        ba.extend(bytes(padding))
    if tobj.ftype == Ftype.DIR:
        if owner_id is None:
            raise TargetObjectError(tobj, "no dirents (internal error)", blob_name=blob_name)
    else:
        owner_id = tobj.first_backpointer
        if owner_id == FILEHANDLE_NULL:
            wrock.logger.warning("%s reconcile %s appears to be orphaned with nlink_effective=%s", wrock, tobj.describe(), tobj.nlink_effective())
    return ba, owner_id

def _obj_reconcile__repack_dirents(wrock, tobj, oblob, blob_name):
    '''
    Helper for obj_reconcile(). Invoked on OBTYPE_DIRENTS.
    oblob is the CLFSObjectBlob.  The purpose here is to replace the
    '..' entry with one that is correct for the current
    tobj.first_backpointer.
    Returns a tuple of (unparsed_data_for_blob, owner_id).
    '''
    ps = ParseState(tobj, oblob.data)
    ps.resume_from_oblob(oblob)
    dirent_list = ps.parse_dirents()
    if len(dirent_list) < 2:
        raise TargetObjectError(tobj, "directory entry list short (length=%d)" % len(dirent_list), blob_name=blob_name)
    dotdot = dirent_list[1]
    if dotdot[0] != '..':
        raise TargetObjectError(tobj, "second entry is not dotdot ('%s')" % dotdot[0], blob_name=blob_name)
    if tobj.first_backpointer == FILEHANDLE_NULL:
        if tobj.filehandle == FILEHANDLE_ROOT:
            dotdot[1] = tobj.filehandle
        else:
            wrock.logger.warning("%s reconcile %s appears to be orphaned", wrock, tobj.describe())
            dotdot[1] = FILEHANDLE_ORPHAN
    else:
        dotdot[1] = tobj.first_backpointer
    ba = bytearray()
    unparse_dirents_list(ba, dirent_list)
    assert len(ba) <= CLFSSegment.FIRST_SEGMENT_BYTES
    return ba, dotdot[1]

class ObjectListManager():
    '''
    Manage state for finding the newest object name
    for an inode.
    '''
    def __init__(self, filehandle):
        self._objid = ObCacheId(filehandle)
        if self._objid.bucketNameVersionIsV1():
            self.prefix = "%08X_%s." % (fnv1_hash(self._objid.bytes), str(self._objid).upper())
            self._name_version = 1
        else:
            self.prefix = "%016X_%s." % (hash64(self._objid), str(self._objid).upper())
            self._name_version = 2
        self.best_name = None
        self.best_version = None

    def see_name(self, wrock, name):
        '''
        An object with the given name is observed.
        '''
        if not name.startswith(self.prefix):
            wrock.logger.warning("%s see_name mismatch prefix '%s' does not match '%s'", wrock, self.prefix, name)
            return
        toks = name.split('.')
        try:
            name_version = int(toks[-1])
        except ValueError:
            wrock.logger.warning("%s cannot parse inode object name '%s'", wrock, name)
            return
        if name_version != self._name_version:
            wrock.logger.warning("%s objid='%s' unexpected name_version '%s' != '%s' name='%s'", wrock, self._objid, name_version, self._name_version, name)
            # we know how to parse this, so keep going
        if name_version == 1:
            if len(toks) != 4:
                wrock.logger.warning("%s objid='%s' unexpected token count for name='%s'", wrock, self._objid, name)
                return
            version_str = toks[1]
        elif name_version == 2:
            if len(toks) != 6:
                wrock.logger.warning("%s objid='%s' unexpected token count for name='%s'", wrock, self._objid, name)
                return
            version_str = toks[3]
        else:
            # OK, we do not know how to handle this
            wrock.logger.warning("%s objid='%s' unhandled name_version=%s", wrock, self._objid, name_version)
            return
        try:
            version = int(version_str)
        except ValueError:
            wrock.logger.warning("%s cannot parse version from object name '%s' version_str='%s'", wrock, name, version_str)
            return
        self._see_name_and_version(name, version)

    def _see_name_and_version(self, name, version):
        '''
        Observe a new name at the given version
        '''
        if (self.best_version is None) or (version > self.best_version):
            self.best_name = name
            self.best_version = version

class ObjectListTracker(ObjectListManager):
    '''
    ObjectListManager variant that tracks older name versions
    '''
    def __init__(self, filehandle):
        super(ObjectListTracker, self).__init__(filehandle)
        self.older_names = set()

    def _see_name_and_version(self, name, version):
        '''
        See parent class
        '''
        if self.best_version is None:
            self.best_name = name
            self.best_version = version
        elif version > self.best_version:
            self.older_names.add(self.best_name)
            self.best_name = name
            self.best_version = version
        else:
            self.older_names.add(name)
